# Traceloop

**Production observability for AI agents**

[![CI](https://github.com/traceloop-ai/traceloop/workflows/CI/badge.svg)](https://github.com/traceloop-ai/traceloop/actions)
[![Go Report Card](https://goreportcard.com/badge/github.com/traceloop-ai/traceloop)](https://goreportcard.com/report/github.com/traceloop-ai/traceloop)
[![Coverage](https://codecov.io/gh/traceloop-ai/traceloop/branch/main/graph/badge.svg)](https://codecov.io/gh/traceloop-ai/traceloop)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

Traceloop provides comprehensive observability and monitoring for AI agents, offering trace collection, visualization, and performance analytics for production AI systems.

## ğŸš€ Features

- **ğŸ” Comprehensive Tracing**: Automatic instrumentation for popular AI frameworks
- **ğŸ“Š Real-time Dashboard**: Beautiful web interface for trace visualization
- **ğŸ¯ Agent-specific Insights**: Purpose-built for AI agent observability
- **âš¡ High Performance**: Minimal overhead with efficient data collection
- **ğŸ”§ Easy Integration**: Simple SDKs for Python, JavaScript, and more
- **ğŸ“ˆ Production Ready**: Scales from development to enterprise deployments

## ğŸ“¦ Quick Start

### Server Installation

**Using Go:**
```bash
go install github.com/traceloop-ai/traceloop/cmd/traceloop@latest
traceloop server
```

**Using Docker:**
```bash
docker run -p 8080:8080 ghcr.io/traceloop-ai/traceloop:latest
```

**Using Homebrew:**
```bash
brew install traceloop-ai/tap/traceloop
traceloop server
```

### Python SDK

```bash
pip install traceloop
```

```python
import traceloop

# Initialize traceloop
traceloop.init(
    endpoint="http://localhost:8080",
    service_name="my-ai-agent"
)

# Automatic tracing with decorators
@traceloop.trace_agent("my-agent")
def my_agent_function(user_input: str) -> str:
    # Your agent logic here
    response = call_llm(user_input)
    return response

@traceloop.trace_llm("gpt-4")
def call_llm(prompt: str) -> str:
    # LLM call logic
    return "AI response"
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Python SDK    â”‚    â”‚   TypeScript    â”‚    â”‚   Other SDKs    â”‚
â”‚                 â”‚    â”‚      SDK        â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ gRPC/HTTP
                         â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚        Traceloop Server         â”‚
          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
          â”‚  â”‚ Trace API   â”‚   Web UI    â”‚ â”‚
          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
          â”‚  â”‚      Storage Layer          â”‚ â”‚
          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ Use Cases

### AI Agent Debugging
```python
@traceloop.trace_agent("research-agent")
def research_agent(query: str):
    # Automatically trace the entire agent execution
    search_results = web_search(query)
    summary = summarize_results(search_results)
    return generate_response(summary)
```

### LLM Performance Monitoring
```python
@traceloop.trace_llm("gpt-4", capture_prompts=True)
def generate_response(context: str):
    response = openai_client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": context}]
    )
    return response.choices[0].message.content
```

### Multi-Agent Coordination
```python
# Trace complex multi-agent workflows
with traceloop.start_trace("multi-agent-task") as trace:
    agent1_result = planning_agent.execute(task)
    agent2_result = execution_agent.execute(agent1_result)
    final_result = review_agent.execute(agent2_result)
```

## ğŸ“Š Dashboard Features

- **Trace Timeline**: Visualize agent execution flows
- **Performance Metrics**: Response times, token usage, error rates
- **Search & Filter**: Find specific traces by agent, time, or attributes
- **Real-time Monitoring**: Live view of running agents
- **Error Analysis**: Detailed error tracking and debugging

## ğŸ”§ Configuration

### Server Configuration

```bash
# Basic server start
traceloop server --port 8080 --host 0.0.0.0

# With custom storage
traceloop server --storage-path ./traceloop-data

# Production configuration
traceloop server \
  --port 8080 \
  --storage-path /var/lib/traceloop \
  --log-level info \
  --metrics-enabled
```

### Python SDK Configuration

```python
import traceloop

# Development setup
traceloop.init(
    endpoint="http://localhost:8080",
    service_name="my-service",
    capture_args=True,
    capture_results=True
)

# Production setup
traceloop.init(
    endpoint="https://traceloop.your-domain.com",
    api_key="your-api-key",
    service_name="production-agent",
    sampling_rate=0.1,  # Sample 10% of traces
    batch_size=100
)
```

## ğŸš€ Development

### Prerequisites

- Go 1.21+
- Python 3.8+
- Node.js 18+ (for web dashboard)
- Make

### Building from Source

```bash
# Clone the repository
git clone https://github.com/traceloop-ai/traceloop.git
cd traceloop

# Initialize dependencies
make init

# Run tests
make test

# Build the binary
make build

# Run locally
make dev
```

### Project Structure

```
traceloop/
â”œâ”€â”€ cmd/traceloop/          # CLI entry point
â”œâ”€â”€ server/                 # Server implementation
â”‚   â”œâ”€â”€ grpc/              # gRPC service for SDKs
â”‚   â”œâ”€â”€ http/              # REST API & web server
â”‚   â””â”€â”€ storage/           # Data storage layer
â”œâ”€â”€ sdk/
â”‚   â”œâ”€â”€ python/            # Python SDK
â”‚   â”œâ”€â”€ typescript/        # TypeScript SDK
â”‚   â””â”€â”€ proto/             # Protocol buffer definitions
â”œâ”€â”€ web/dashboard/         # React dashboard
â”œâ”€â”€ examples/              # Usage examples
â””â”€â”€ docs/                  # Documentation
```

## ğŸ§ª Examples

### LangChain Integration

```python
import traceloop
from langchain.agents import create_openai_functions_agent

# Initialize traceloop
traceloop.init(service_name="langchain-agent")

# Your existing LangChain code works unchanged
# Traceloop automatically instruments LangChain components
agent = create_openai_functions_agent(llm, tools, prompt)
result = agent.invoke({"input": "What's the weather like?"})
```

### OpenAI Integration

```python
import traceloop
import openai

traceloop.init(service_name="openai-app")

# Automatic instrumentation for OpenAI calls
@traceloop.trace
def chat_with_ai(message: str):
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": message}]
    )
    return response.choices[0].message.content
```

### CrewAI Integration

```python
import traceloop
from crewai import Agent, Task, Crew

traceloop.init(service_name="crewai-workflow")

# Define agents with automatic tracing
researcher = Agent(
    role='Researcher',
    goal='Find relevant information',
    backstory='Expert researcher'
)

# Crew execution is automatically traced
crew = Crew(agents=[researcher], tasks=[task])
result = crew.kickoff()
```

## ğŸ“š Documentation

- [Installation Guide](docs/installation.md)
- [Python SDK Reference](docs/python-sdk.md)
- [Server Configuration](docs/server-config.md)
- [Dashboard Guide](docs/dashboard.md)
- [API Reference](docs/api.md)

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

### Development Workflow

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Make your changes and add tests
4. Run the test suite: `make test`
5. Commit your changes: `git commit -m 'Add amazing feature'`
6. Push to your fork: `git push origin feature/amazing-feature`
7. Open a Pull Request

## ğŸ“ License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support

- ğŸ“– [Documentation](https://docs.traceloop.ai)
- ğŸ’¬ [Discord Community](https://discord.gg/traceloop)
- ğŸ› [GitHub Issues](https://github.com/traceloop-ai/traceloop/issues)
- ğŸ“§ [Email Support](mailto:support@traceloop.ai)

## ğŸŒŸ Roadmap

- [ ] **Multi-tenant support** - Enterprise-ready isolation
- [ ] **Advanced analytics** - ML-powered insights
- [ ] **Custom dashboards** - Build your own views
- [ ] **Alerting & notifications** - Proactive monitoring
- [ ] **More SDK languages** - Java, Rust, and more
- [ ] **Cloud deployment** - Managed service offering

## â­ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=traceloop-ai/traceloop&type=Date)](https://star-history.com/#traceloop-ai/traceloop&Date)

---

Built with â¤ï¸ for the AI community
